{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"_nvu0wyhS2aX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713555559523,"user_tz":240,"elapsed":984,"user":{"displayName":"D J","userId":"13675345049761676965"}},"outputId":"9fd8de60-f225-4926-829c-1434aaff5358"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vag2WYR6yTOC"},"outputs":[],"source":["import tensorflow as tf\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","#mpl.rcParams['figure.figsize'] = (16, 16)\n","mpl.rcParams['axes.grid'] = False\n","mpl.rcParams['figure.figsize'] = (4, 4)"]},{"cell_type":"markdown","metadata":{"id":"wiTHY8dqxzx7"},"source":["Let's load the pretrained MobileNetV2 model and the ImageNet class names."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nqhk2vYx6Ag0"},"outputs":[],"source":["pretrained_model = tf.keras.applications.MobileNetV2(include_top=True,\n","                                                     weights='imagenet')\n","pretrained_model.trainable = False\n","\n","# ImageNet labels\n","decode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f2cLrJH0zpfC"},"outputs":[],"source":["# Helper function to preprocess the image so that it can be inputted in MobileNetV2\n","def preprocess(image):\n","  image = tf.cast(image, tf.float32)\n","  image = tf.image.resize(image, (224, 224))\n","  image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n","  image = image[None, ...]\n","  return image\n","\n","# Helper function to extract labels from probability vector\n","def get_imagenet_label(probs):\n","  return decode_predictions(probs, top=1)[0][0]"]},{"cell_type":"markdown","metadata":{"id":"iEZaMVFgSUA-"},"source":["## Original image\n","Let's use a sample image of a [Labrador Retriever](https://commons.wikimedia.org/wiki/File:YellowLabradorLooking_new.jpg) by Mirko [CC-BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/) from Wikimedia Common and create adversarial examples from it. The first step is to preprocess it so that it can be fed as an input to the MobileNetV2 model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wpYrQ4OQSYWk"},"outputs":[],"source":["image_path = tf.keras.utils.get_file('YellowLabradorLooking_new.jpg', 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg')\n","\n","image_raw = tf.io.read_file(image_path)\n","image = tf.image.decode_image(image_raw)\n","\n","image = preprocess(image)\n","image_probs = pretrained_model.predict(image)"]},{"cell_type":"code","source":["plt.figure()\n","plt.imshow(image[0] * 0.5 + 0.5)  # To change [-1, 1] to [0,1]\n","_, image_class, class_confidence = get_imagenet_label(image_probs)\n","plt.title('{} : {:.2f}% Confidence'.format(image_class, class_confidence*100))\n","plt.show()"],"metadata":{"id":"4n_bm69336_X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kElVTbF690CF"},"source":["## Create the adversarial image\n","\n","### Implementing fast gradient sign method\n","The first step is to create perturbations which will be used to distort the original image resulting in an adversarial image. As mentioned, for this task, the gradients are taken with respect to the image."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FhZxlOnuBCVr"},"outputs":[],"source":["loss_object = tf.keras.losses.CategoricalCrossentropy()\n","\n","def create_adversarial_pattern_PATCH(input_image, input_label, patch_size):\n","  with tf.GradientTape() as tape:\n","    tape.watch(input_image)\n","    prediction = pretrained_model(input_image)\n","    loss = loss_object(input_label, prediction)\n","\n","  # Get the gradients of the loss w.r.t to the input image.\n","  gradient = tape.gradient(loss, input_image)\n","\n","  # Get the sign of the gradients to create the perturbation\n","  signed_grad = tf.sign(gradient)\n","\n","  # Apply the perturbation only on the specified patch size\n","  height, width, channels = signed_grad.shape[1:4]\n","  center_h = height // 2\n","  center_w = width // 2\n","  patch_radius = patch_size // 2\n","  perturbation_mask = np.zeros_like(signed_grad[0])\n","  perturbation_mask[center_h - patch_radius:center_h + patch_radius + 1, center_w - patch_radius:center_w + patch_radius + 1, :] = 1\n","  signed_grad = signed_grad * perturbation_mask\n","\n","  return signed_grad"]},{"cell_type":"markdown","metadata":{"id":"RbuftX0eSlDQ"},"source":["The resulting perturbations can also be visualised."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rVjnb6M7Smv4"},"outputs":[],"source":["# Get the input label of the image.\n","labrador_retriever_index = 905\n","label = tf.one_hot(labrador_retriever_index, image_probs.shape[-1])\n","label = tf.reshape(label, (1, image_probs.shape[-1]))\n","\n","patch_size = 60\n","perturbations = create_adversarial_pattern_PATCH(image, label, patch_size)\n","plt.imshow(perturbations[0] * 0.5 + 0.5);  # To change [-1, 1] to [0,1]"]},{"cell_type":"markdown","metadata":{"id":"DKKSFHjwCyQH"},"source":["Let's try this out for different values of epsilon and observe the resultant image. You'll notice that as the value of epsilon is increased, it becomes easier to fool the network. However, this comes as a trade-off which results in the perturbations becoming more identifiable."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dBtG0Kl5SspV"},"outputs":[],"source":["def display_images(images, descriptions):\n","  num_images = len(images)\n","  num_rows = (num_images + 3) // 4  # Calculate the number of rows needed\n","\n","  fig, axs = plt.subplots(num_rows, 4, figsize=(16, 4 * num_rows))  # Create a figure with multiple subplots\n","\n","  for i, (image, description) in enumerate(zip(images, descriptions)):\n","    row, col = i // 4, i % 4  # Calculate the row and column indices\n","    _, label, confidence = get_imagenet_label(pretrained_model.predict(image))\n","\n","    axs[row, col].imshow(image[0] * 0.5 + 0.5)\n","    axs[row, col].set_title('{} \\n {} : {:.2f}% Confidence'.format(description, label, confidence * 100))\n","    axs[row, col].axis('off')\n","\n","  # Remove any unused subplots\n","  for i in range(num_images, num_rows * 4):\n","    row, col = i // 4, i % 4\n","    fig.delaxes(axs[row, col])\n","\n","  plt.tight_layout()\n","  plt.show()"]},{"cell_type":"code","source":["epsilons = [0, 0.01, 0.1, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 1.0]\n","#epsilons = [0.15, 0.20, 0.25, 0.50]\n","descriptions = [('Epsilon = {:0.3f}'.format(eps) if eps else 'Input') for eps in epsilons]\n","\n","images = [image]\n","descriptions = ['Input']  # Initialize the descriptions list with 'Input'\n","\n","for eps in epsilons:\n","  adv_x = image + eps * perturbations\n","  adv_x = tf.clip_by_value(adv_x, -1, 1)\n","  images.append(adv_x)\n","  descriptions.append(f'Epsilon = {eps:.3f}')  # Add the description for the current epsilon\n","\n","display_images(images, descriptions)"],"metadata":{"id":"MqFL6MlBv6xQ"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1Lx6nAPzYQ9ipLzBRpLCJIuH-ZEi-rfCV","timestamp":1713557967556},{"file_id":"155ja1IhUhtHcfsJab6XtfvoYmXQFgQ2L","timestamp":1713210709198},{"file_id":"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/adversarial_fgsm.ipynb","timestamp":1713208491573}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}